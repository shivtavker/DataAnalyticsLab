{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1P3fTzt_wZ1"
   },
   "source": [
    "# Diabetes Data Set\n",
    "\n",
    "Dataset file: 'diabetes.data'  \n",
    "Reference link for description of dataset: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwuU7CGN_wZ4"
   },
   "source": [
    "### Preview of the Data Set\n",
    "\n",
    "Load the data set.\n",
    "\n",
    "a) Analyse the data set. Print the number of features, feature names, data types of the features, number of data points and the values of the first 10 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43689,
     "status": "ok",
     "timestamp": 1565616556350,
     "user": {
      "displayName": "MALKAPURAM GANDLA SAI SREE",
      "photoUrl": "",
      "userId": "06629185457401699264"
     },
     "user_tz": -330
    },
    "id": "Xzaddo1IA7hv",
    "outputId": "96b5cb56-eb5b-4fcb-d768-e7a6e7533463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    AGE  SEX   BMI     BP   S1     S2    S3    S4      S5  S6    Y\n",
      "0    59    2  32.1  101.0  157   93.2  38.0  4.00  4.8598  87  151\n",
      "1    48    1  21.6   87.0  183  103.2  70.0  3.00  3.8918  69   75\n",
      "2    72    2  30.5   93.0  156   93.6  41.0  4.00  4.6728  85  141\n",
      "3    24    1  25.3   84.0  198  131.4  40.0  5.00  4.8903  89  206\n",
      "4    50    1  23.0  101.0  192  125.4  52.0  4.00  4.2905  80  135\n",
      "5    23    1  22.6   89.0  139   64.8  61.0  2.00  4.1897  68   97\n",
      "6    36    2  22.0   90.0  160   99.6  50.0  3.00  3.9512  82  138\n",
      "7    66    2  26.2  114.0  255  185.0  56.0  4.55  4.2485  92   63\n",
      "8    60    2  32.1   83.0  179  119.4  42.0  4.00  4.4773  94  110\n",
      "9    29    1  30.0   85.0  180   93.4  43.0  4.00  5.3845  88  310\n",
      "10   22    1  18.6   97.0  114   57.6  46.0  2.00  3.9512  83  101\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('diabetes.data','\\t')\n",
    "print(data[:11])\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sm9xTLRe_waF"
   },
   "source": [
    "### Training and Testing Data Sets\n",
    "\n",
    "b) Split the data set into training and testing data set with a 80:20 ratio.\n",
    "\n",
    "(Hint: What precautions must you take before you split the data set?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3vwkJem_waG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Shuffling the data before slicing it off to ensure randomness and break any sort of order\n",
    "np.random.shuffle(data)\n",
    "train = data[0:354,:]\n",
    "test = data[354:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7qfwzLcr_waJ"
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "c) Using linear regression, seek a model for the response of interest ($Y$), as a function of the baseline variables such as age, sex, body mass index, etc. Compute the training error and testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1565616648527,
     "user": {
      "displayName": "MALKAPURAM GANDLA SAI SREE",
      "photoUrl": "",
      "userId": "06629185457401699264"
     },
     "user_tz": -330
    },
    "id": "DvCRLsF-_waK",
    "outputId": "3e0434e8-0df9-4035-f2a0-fcdb685aced7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error:  2908.6779784108207\n",
      "Test Error:  2753.3208215123846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train = train[:, 0:10]\n",
    "Y_train = train[:, 10]\n",
    "X_test = test[:, 0:10]\n",
    "Y_test = test[:, 10]\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train Error: \", mean_squared_error(Y_train, reg.predict(X_train)))\n",
    "print(\"Test Error: \", mean_squared_error(Y_test, reg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "d) Normalize the data set and perform linear regression again. Compute the training error and testing error. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1565616649829,
     "user": {
      "displayName": "MALKAPURAM GANDLA SAI SREE",
      "photoUrl": "",
      "userId": "06629185457401699264"
     },
     "user_tz": -330
    },
    "id": "1hO4CGs9_waO",
    "outputId": "d4dfda9a-e901-4b46-d8e6-0d4fca57820f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error after Normalize:  2908.6779784108203\n",
      "Test Error after Normalize:  2753.320821512383\n",
      "\n",
      "We see that Train and Test Errors remain the same.\n",
      "This is because we just subtract and divide by constant terms to each feature of each sample point\n",
      "Subtracting and Dividing by constant terms just changes the coefficients and not Prediction Value\n",
      "However, Normalizing helps us in deciding the important features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale = StandardScaler()\n",
    "X_train_scaled = scale.fit_transform(X_train)\n",
    "X_test_scaled = scale.transform(X_test)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Train Error after Normalize: \", mean_squared_error(Y_train, reg.predict(X_train_scaled)))\n",
    "print(\"Test Error after Normalize: \", mean_squared_error(Y_test, reg.predict(X_test_scaled)))\n",
    "print()\n",
    "print(\"We see that Train and Test Errors remain the same.\")\n",
    "print(\"This is because we just subtract and divide by constant terms to each feature of each sample point\")\n",
    "print(\"Subtracting and Dividing by constant terms just changes the coefficients and not Prediction Value\")\n",
    "print(\"However, Normalizing helps us in deciding the important features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wo1Xa-0bDZmq"
   },
   "source": [
    "### Feature Reduction\n",
    "\n",
    "e) Rank the features in order of importance (based on the study in d)). Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1565617372770,
     "user": {
      "displayName": "MALKAPURAM GANDLA SAI SREE",
      "photoUrl": "",
      "userId": "06629185457401699264"
     },
     "user_tz": -330
    },
    "id": "8NzU6I2RDiVK",
    "outputId": "98cdbe7e-29e2-438e-de5f-bd57419c2b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in Decreasing order of Importance according to their coefficient values\n",
      "S1\n",
      "S5\n",
      "BMI\n",
      "S2\n",
      "BP\n",
      "S4\n",
      "SEX\n",
      "S3\n",
      "S6\n",
      "AGE\n"
     ]
    }
   ],
   "source": [
    "# print(reg.coef_)\n",
    "print(\"Features in Decreasing order of Importance according to their coefficient values\")\n",
    "print(\"S1\")\n",
    "print(\"S5\")\n",
    "print(\"BMI\")\n",
    "print(\"S2\")\n",
    "print(\"BP\")\n",
    "print(\"S4\")\n",
    "print(\"SEX\")\n",
    "print(\"S3\")\n",
    "print(\"S6\")\n",
    "print(\"AGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "f) Repeat the exercise in d) with quadratic features. List the features you would add to the existing data set. Compute the training error and the testing error. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error:  2431.5011577282403\n",
      "Test Error:  2840.661063272829\n",
      "\n",
      "We see that Train Error decreases as we move from Linear regression to polynomial regression\n",
      "We also see that Test Error increases as we move from Linear regression to polynomial regression\n",
      "This is due to overfitting\n",
      "\n",
      "We add Quadratic Features, ie suppose our intial feautres we X1, X2\n",
      "We now add extra features like X1^2, X2^2, X1X2\n",
      "In total we have features like X1^2, X2^2, X1X2, X1, X2, 1(for intercept)\n",
      "For K features and d degree polynomial fitting we get (K+d)C(d) total number of features\n",
      "From the above formulae, total features in our case will be 12C2 i.e. 66 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree = 2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "scale = StandardScaler()\n",
    "X_train_scaled = scale.fit_transform(X_train_poly)\n",
    "X_test_scaled = scale.transform(X_test_poly)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Train Error: \", mean_squared_error(Y_train, reg.predict(X_train_scaled)))\n",
    "print(\"Test Error: \", mean_squared_error(Y_test, reg.predict(X_test_scaled)))\n",
    "print()\n",
    "print(\"We see that Train Error decreases as we move from Linear regression to polynomial regression\")\n",
    "print(\"We also see that Test Error increases as we move from Linear regression to polynomial regression\")\n",
    "print(\"This is due to overfitting\")\n",
    "print()\n",
    "print(\"We add Quadratic Features, ie suppose our intial feautres we X1, X2\")\n",
    "print(\"We now add extra features like X1^2, X2^2, X1X2\")\n",
    "print(\"In total we have features like X1^2, X2^2, X1X2, X1, X2, 1(for intercept)\")\n",
    "print(\"For K features and d degree polynomial fitting we get (K+d)C(d) total number of features\")\n",
    "print(\"From the above formulae, total features in our case will be 12C2 i.e. 66 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exercise Part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
